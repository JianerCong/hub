
\section{Consideration}

\subsection{The logical architecture}

\cSay{ OpenStack uses a microservice architecture. So it's composed of many many services. }

About this services:

\begin{itemize}
\item Most of them of them are written in \cola{Python}.
\item All of them provide a \cola{REST API}.
\item Each service may be implemented as different components. The components
  communicate with each other using a \cola{message queue}.
\end{itemize}

\subsubsection{Keystone - identity management}

\cSay{ Keystone is the identity management service. It's the simplest service in
  OpenStack.}

It provides a REST API for authentication and authorization. It also provides a
service catalog, which is a list of all the services.

\dSay{ Why is it keeping it ?}

\cSay{ I think probably because it needs to know which services are accessible
  for a user anyway.}


\subsubsection{Swift - object storage}

\leftSay{\svgOs{swift}}{My name is \Cola{Swift}.

  I am 
  the object storage service (an object store).

  To store an object, I split the object into chunks and stores them in different containers redundantly.

  I implement a distributed, eventually consistent object/blob store that is accessible via HTTP/HTTPS.
}

The connection of swift is shown in the \cref{fig:production-storage-swift}.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{/home/me/Pictures/opstk/production-storage-swift.png}
  \caption{swift storage overview (Courtesy: \url{https://docs.openstack.org/openstack-ansible/2023.1/reference/architecture/storage-arch.html}))}
  \label{fig:production-storage-swift}
\end{figure}

\colz{
  The \colZt{swift-proxy} service is accessed by \cola{clients} via the load balancer on the
  management network (\colZt{br-mgmt}). The \colZt{swift-proxy} service communicates with the
  \cola{
    Account, Container, and Object services on the Object Storage hosts via the
    storage network(\texttt{br-storage}).
  }
  Replication between the Object Storage hosts is
  done via the replication network (\colZt{br-repl}).
}


Some of the benefits of this design are:

\begin{itemize}
\item It has no central brain, and indicates no \Cola{Single Point of Failure (SPOF)}.
\item It has \cola{auto-revovery}.
\item It has inexpensive hardware that can be used for redundant storage clusters.
\end{itemize}

\dSay{ Wow, \cola{auto-recovey} is cool.}

\FloatBarrier

\subsubsection{Cinder - block storage}

\dSay{ What if I just want a block of bytes ? For example, for the volume of a VM.}

\cSay{ Then you can use Cinder. It provides raw volumes that can be used as hand
  disks in VMs.
}

\colz{
  The Block Storage (cinder) service manages volumes on storage devices in an
  environment. In a production environment, the device presents storage via a
  storage protocol (for example, \colZ{NFS, iSCSI, or Ceph RBD}) to a storage network
  (\colZt{br-storage}) and a storage management API to the management network (\colZ{br-mgmt}).
  Instances are connected to the volumes via the storage network by the
  hypervisor on the Compute host.
}

The connection of cinder is shown in the \cref{fig:production-storage-cinder}.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{/home/me/Pictures/opstk/production-storage-cinder.png}
  \caption{cinder storage overview (Courtesy: \url{https://docs.openstack.org/openstack-ansible/2023.1/reference/architecture/storage-arch.html}))}
  \label{fig:production-storage-cinder}
\end{figure}

\cref{fig:production-storage-cinder} shows the following steps:
\colz{
  \begin{enumerate}
  \item A volume is created by the assigned \colZ{\texttt{cinder-volume} service} using the
    appropriate \Cola{cinder driver}. The volume is created by using an API that is
    presented to the \colZ{management network}.
  \item After the volume is created, the \colZ{nova-compute service} connects the
    Compute host hypervisor to the volume via the \colZ{storage network}.
  \item After the hypervisor is connected to the volume, it presents the volume
    as a local hardware device to the instance.
  \end{enumerate}
}

\dSay{Oh, so the ``cloud storage'' technology needed is something like NFS or iSCSI ?}

\cSay{Yes}

\dSay{
  Okay, so what are the available \cola{cinder drivers} ?
}

\cSay{
  The \Cola{LVMVolumeDriver} is designed as a reference driver implementation.
  But it's recommended for production use by the official documentation. Because
  once the host is down, all the managed volumes become unavailable.

  Also, Upgrading the operating system packages (e.g. kernel or iSCSI) on the
  server causes storage connectivity outages because the iSCSI service restarts.
}

\dSay{
  Okay so what are the other drivers ?
}

\cSay{
  It seems like the doc recommands commercial drivers... Also, these services
  can not be containerized.

}

\colz{
  When
  \colZ{
    the cinder-volume service is configured to manage volumes
    on the same back end from multiple hosts or containers
  }, \cola{one service} is
  scheduled to manage the life cycle of the volume until an alternative service
  is assigned to do so. This assignment can be made through the \Cola{cinder-manage
  CLI tool}.
}


Some of the features of Cinder are:

\begin{itemize}
\item You can create snapshots of volumes.
\item You can create backups of volumes.
\end{itemize}

Also, like Keystone, different storage backends can be plugged into Cinder such
as from IBM, NetApp, Nexenta, and VMware.

\dSay{ What are these ?}

\cSay{ I don't know. I think they are companies that provide storage....}

\begin{tcolorbox}
\cSay{ Cinder replaces the old \Cola{Nova Volume service}. }
\end{tcolorbox}



\subsubsection{Glance - image storage}
\label{sec:glance}

\leftSay{\svgOs[0.2\linewidth]{glance}}{My name is \Cola{Glance}.

  I am the image storage service.

  I can store images on a variety of storage back ends supported by the \cola{\texttt{glance\_store\_drivers}}.
}

\cSay{
  \colz{

    When the \cola{File System store} is used, the Image service has \colz{no mechanism of
      its own to replicate the image between Image service hosts}. We recommend
    using a \colZ{shared storage back end} (via a file system mount) to ensure that all
    glance-api services have access to all images. Doing so prevents losing
    access to images when an infrastructure (control plane) host is lost.

  }
}

The connection of glance is shown in the \cref{fig:production-storage-glance}.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{/home/me/Pictures/opstk/production-storage-glance.png}
  \caption{glance storage overview (Courtesy: \url{https://docs.openstack.org/openstack-ansible/2023.1/reference/architecture/storage-arch.html}))}
  \label{fig:production-storage-glance}
\end{figure}
\FloatBarrier

In \cref{fig:production-storage-glance}, the \cola{glance-api} service has the
following steps:
\colz{
  \begin{enumerate}
  \item When a \colZ{client requests an image}, the glance-api service accesses the
    appropriate store on the storage device over the \colZ{storage network (\texttt{br-storage})}
    and pulls it into its \colZ{cache}. When the same image is requested again, it is
    given to the client directly from the cache.
  \item When an \colZ{instance is scheduled} for creation on a Compute host, the
    \colZ{nova-compute service } requests the image from the \colZt{glance-api} service
    over the \colZ{management network (\texttt{br-mgmt})}.
  \item After the image is retrieved, the nova-compute service stores the image
    in its \colZ{own image cache}. When another instance is created with the same
    image, the image is retrieved from the local base image cache.
  \end{enumerate}
}


\subsubsection{Manila - file share (since \cola{Juno})}

\cSay{ Manila is the file share service. It provides a remote file system. In
  operation, it resembles the \Cola{Network File System (NFS)} or \Cola{SAMBA}
  storage service that you can find in Linux.}


\dSay{Oh, I know SAMBA. I remember it's how linux can access windows file.}
\cSay{Yes, it's a file sharing protocol.}


In contrast to Cinder, it resembles the \Cola{Storage Area Network (SAN)}
service. In fact, NFS and SAMBA or the \Cola{Common Internet File System (CIFS)}
are supported as backend drivers. Manila provides the orchestration of shares on
the share services.

\dSay{What do you mean by \cola{orchestration of shares} ?}

\cSay{ I think it means that it will take care of the shares when they are read and written by different services.}

\dSay{Oh, so what's the difference between Swift, Cinder and Manila ?}

\cSay{ Let's see a table:}

\begin{table}[htbp]
  \centering
  \begin{tabularx}{0.8\textwidth}{XXXX}
    \textbf{Specification} & \textbf{Swift} & \textbf{Cinder} & \textbf{Manila} \\
    \hline
    \textbf{Access mode} & REST API & Block device & File system \\
    \textbf{Multi-access} & \emoji{check-mark-button} & \emoji{cross-mark} & \emoji{check-mark-button} \\
    \textbf{Persistent} & \emoji{check-mark-button} & \emoji{check-mark-button} & \emoji{check-mark-button} \\
    \textbf{Accessibility} & Anywhere & Within single VM & Within multiple VMs \\
  \end{tabularx}
\end{table}

\subsubsection{Nova - compute service}
\leftSay{\svgOs{nova}}{My name is \Cola{Nova}. I am the compute service, the most complex service in OpenStack.}

\dSay{Wait... I remember you said that Nova is replaced by Cinder ?}

\cSay{ Only the \Cola{volume service }is replaced by Cinder. }

\dSay{Oh, I see. So how it's complex ?}

\cSay{ First, it needs to interact with many other services. Second, its internal components are complex, because it needs to respond to user requests for running VMs.}

Let's decompose it into its components:

\begin{description}
\item[nova-api] : It accepts and responds to user requests of creating instances
  (VMs) via the OpenStack API or the EC2 API.
\item[nova-compute] : It creates and terminates VMs via the hypervisor's APIs
  (XenAPI for XenServer, libvirt for KVM and the VMware API for VMware).
\item[nova-network] : It provides network connectivity for VMs. It's deprecated
  in favor of Neutron \emoji{litter-in-bin-sign}.
\item[nova-scheduler] : It schedules VMs to run on different compute nodes.
  (\emoji{parrot} : So here's where OpenStack find the best node to run a VM ?,
  \emoji{turtle} : Yes).
\item[nova-conductor] : It provides database access for the other components.
  (\emoji{parrot} : Why bother adding access control ? \emoji{turtle} : This is
  for security. If some nodes are compromised, we can't just let them access the
  database however they want).
\end{description}

\cSay{

  Compute host also needs some RAM (Ephemeral storage). Usually it's on the
  compute host itself, but it can also (recommended) to use a \Cola{shared
    storage subsystem} instead.

}

\colz{

  Using a shared storage subsystem also allows the recovery of instances
  when a Compute host goes offline. \Cola{The administrator is able to evacuate the
  instance to another Compute host and boot it up again}. The following diagram
  illustrates the interactions between the storage device, the Compute host, the
  hypervisor, and the instance.
}


The connection of nova RAM is shown in the \cref{fig:production-storage-nova}.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{/home/me/Pictures/opstk/production-storage-nova.png}
  \caption{glance storage overview (Courtesy: \url{https://docs.openstack.org/openstack-ansible/2023.1/reference/architecture/storage-arch.html}))}
  \label{fig:production-storage-glance}
\end{figure}
\FloatBarrier

In \cref{fig:production-storage-glance}, the \cola{glance-api} service has the



\subsubsection{Neutron - networking services}


\rightSay{\svgOs{neutron}}{
  My name is \Cola{Nova}. I am the compute service, the most complex service in OpenStack.
  My name is \Cola{Neutron}.I provide a \Cola{Network
    as Service (NaaS)} capability between \cola{interface devices (routers,
    switches, load balancers, VPNs, firewalls)} managed by other OpenStack
  services (such as Nova) or external networks.
}

\cSay{ \Cola{Neutron} has the following features:}

\begin{itemize}
\item It allows users to create their own networks and attach interfaces to them.
\item Its backend is extensible, so it can take advantage of commodity gear or
  vandor-supported equipment.
\end{itemize}

Neutron has three main components:

\begin{description}
\item[server] : It accepts API requests and routes them to the appropriate
  \cola{plugin} for action.
\item[plugins] : They perform the actual work for the orchestration of backend devices such as
  \begin{itemize}
  \item plugging in or unplugging ports
  \item creating networks and subnets
  \item IP addressing.
  \end{itemize}
\item[agents] : They run on compute and network nodes. They receive commands
  from the plugins on the server and bring the changes into effect on the
  individual nodes. (\emoji{parrot}: Oh, so agent is like a client ?
  \emoji{turtle}: Yes).

  There're different types of agents. They provide different services. For example:
  \begin{description}
  \item[L3 agents] : They provide routing services.
  \item[Open vSwitch agents] : They provide layer 2 connectivity, by plugging
    and unplugging ports onto \Cola{Open vSwitch(OVS)} bridges.
  \end{description}
  \emoji{parrot}: What is OVS ? \emoji{turtle}: It's a virtual switch.

\end{description}


\section{Networking requirements}

\cSay{
  Due to the use of containers to isolate the OpenStack services, special network
  configuration is needed. In particular \Cola{OpenStack Ansible} uses
  \Cola{linux bridges} to provide network connectivity to the LXC containers.
}

\dSay{What is a linux bridge ?}

\cSay{It's a virtual switch that can be used to connect multiple network
  interfaces together. You can see what are on your system by running the command
  \texttt{ip link show}.
}