** ansible
*** hi
**** hi
1. install
#+begin_src bash
python3 -m pip install --user ansible
#+end_src

#+begin_src bash
  # just install in the venv
  pip install ansible
#+end_src

2. create an inventory by adding the IP address in ~/etc/ansible/hosts~.

   ü¶ú : I don't want to pollute the  ~/etc/~ folder. Is there any other ways to
   do it ?
   üê¢ : Yeah, see next section.

#+begin_src yaml
  [myvirtualmachines]
  192.0.2.50
  192.0.2.51
  192.0.2.52 
#+end_src

3. verify the hosts

ansible all --list-hosts

4. Set up SSH connections so Ansible can connect to the managed hosts.

   a. Add the public SSH key to the `authorized_keys` file on each remote
   system.
   b. Test the SSH connections.

   #+begin_src bash
     ssh me@192.0.2.50
   #+end_src

   If the username on the control node is different, you need to pass the ~-u~
   option with the ~ansible~ command.

5. Ping the managed hosts.

   #+begin_src bash
     ansible all -m ping
   #+end_src
**** create inventory
1. Create the inventory file ~inventory.yaml~ in any folder:
#+begin_src yaml
  myvms:
    hosts:
      vm01:
        ansible_host: 192.0.2.50
      vm02:
        ansible_host: 192.0.2.51
      vm03:
        ansible_host: 192.0.2.52
#+end_src

Add a new group for your hosts then specify the IP address or fully qualified
domain name (FQDN) of managed node in the group with ~ansible_host~ field.

2. Verify your inventory.

   #+begin_src bash
     ansible-inventory -i inventory.yaml --list
   #+end_src

3. ping the managed hosts.

   #+begin_src bash
     ansible myvms -i inventory.yaml -m ping
   #+end_src

üê¢ : Note:

+ Ensure that group names are meaningful and unique. They are case-sensitive.
+ Invalid group names include "I have space", "I-have-hyphens", "1IhavePrecedingNum"
**** variables
Variables are like the args passed to the ansible command.

They can be local

#+begin_src yaml
 webservers:
  hosts:
    webserver01:
      ansible_host: 192.0.2.140
      http_port: 80
    webserver02:
      ansible_host: 192.0.2.150
      http_port: 443 
#+end_src

Or group-scoped

#+begin_src yaml
  webservers:
  hosts:
    webserver01:
      ansible_host: 192.0.2.140
      http_port: 80
    webserver02:
      ansible_host: 192.0.2.150
      http_port: 443
  vars:
    ansible_user: my_server_user
#+end_src
**** playbook
üê¢ : playbook is like the startup script for the managed hosts.

+ play :: an ordered list of *tasks* to execute against nodes in an *inventory*.
+ task :: A list of one or more *modules* that defines the operations to be
  performed by Ansible. that Ansible performs.
+ module :: a unit of code or binary that Ansible runs on managed nodes.

1. Create the playbook to print "Hello world".

playbook.yaml:
#+begin_src yaml
  - name: My first play
    hosts: virtualmachines
    tasks:
     - name: Ping my hosts
       ansible.builtin.ping:

     - name: Print message
       ansible.builtin.debug:
         msg: Hello world
#+end_src

2. Run the playbook.

inventory.yaml:
   #+begin_src bash
     ansible-playbook -i inventory.yaml playbook.yaml
   #+end_src
*** inventory
**** implicit group: ~all~
All nodes have at least two groups: ~all~ and its own group, (or ~ungrouped~
group, for orphan nodes).
ü¶ú : Oh, so we can just do something like ping all the nodes.
#+begin_src bash
  ansible all -i my-first-inventory.yaml -m ping
#+end_src

*** vars
**** precedence

1. command line values (for example, -u my_user, these are not variables)
2. role defaults (defined in role/defaults/main.yml) 1
3. inventory file or script group vars 2
4. inventory group_vars/all 3
5. playbook group_vars/all 3
6. inventory group_vars/* 3
7. playbook group_vars/* 3
8. inventory file or script host vars 2
9. inventory host_vars/* 3
10. playbook host_vars/* 3
11. host facts / cached set_facts 4
12. play vars
13. play vars_prompt
14. play vars_files
15. role vars (defined in role/vars/main.yml)
16. block vars (only for tasks in block)
17. task vars (only for the task)
18. include_vars
19. set_facts / registered vars
20. role (and include_role) params
21. include params
22. extra vars (for example, -e "user=my_user")(always win precedence)
**** define in play
#+begin_src yaml
 - hosts: webservers
  vars:
    http_port: 80 
#+end_src
**** define in *var files*
hi.yaml
#+begin_src yaml
---

- hosts: all
  remote_user: root
  vars:
    favcolor: blue
  vars_files:
    - /vars/external_vars.yml

  tasks:

  - name: This is just a placeholder
    ansible.builtin.command: /bin/echo foo
#+end_src
/vars/external_vars.yaml
#+begin_src yaml
  ---
# in the above example, this would be vars/external_vars.yml
somevar: somevalue
password: magic
#+end_src
**** define at runtime
#+begin_src bash
    ansible-playbook release.yml --extra-vars "version=1.23.45 other_variable=foo"

    ansible-playbook release.yml --extra-vars '{"version":"1.23.45","other_variable":"foo"}'
    ansible-playbook arcade.yml --extra-vars '{"pacman":"mrs","ghosts":["inky","pinky","clyde","sue"]}'
    ansible-playbook script.yml --extra-vars "{\"dialog\":\"He said \\\"I just can\'t get enough of those single and double-quotes"\!"\\\"\"}"

    ansible-playbook release.yml --extra-vars "@some_file.json"
    ansible-playbook release.yml --extra-vars "@some_file.yaml"
#+end_src
**** scope
You can decide where to set a variable based on the scope you want that value to have. Ansible has three main scopes:

+ Global :: set by config, environment variables and the command line
+ Play :: each play and contained structures, vars entries (vars; vars_files; vars_prompt), role defaults and vars.
+ Host :: variables directly associated to a host, like ~inventory~, ~include_vars~, ~facts~ or ~registered task outputs~
**** set for role
roles/common_settings/vars/main.yml
#+begin_src yaml
  roles:
   - role: common_settings
   - role: something
     vars:
       foo: 12
   - role: something_else
#+end_src
**** worry?
Instead of worrying about variable precedence, we encourage you to think about
how easily or how often you want to override a variable when deciding where to
set it.

üê¢ : If you are not sure what other variables are defined, and you need a
particular value, use --extra-vars (-e) to override all other variables.

** OpenStack Ansible
*** all in one [for testing]
ü¶ú : All the following commands should be run with ~sodo~.
1. Prepare the host
   #+begin_src bash
     apt update
     apt dist-upgrade
     reboot
   #+end_src

2. bootstrap Ansible and the required roles.
#+begin_src bash
  git clone https://opendev.org/openstack/openstack-ansible /opt/openstack-ansible
  cd /opt/openstack-ansible
#+end_src

3. checkout the appropriate branch
   #+begin_src bash
     git checkout 27.0.1
   #+end_src

4. run the bootstrap-ansible script
   #+begin_src bash
     TMPDIR=/var/tmp scripts/bootstrap-ansible.sh
   #+end_src
  ü¶ú : There's a lot of configurable options here, but I am gonna stick to the
   default for now.

5. run the bootstrap-aio script
    #+begin_src bash
      scripts/bootstrap-aio.sh
    #+end_src

6. run the playbooks
   #+begin_src bash
     cd /opt/openstack-ansible/playbooks
     openstack-ansible setup-hosts.yml
     openstack-ansible setup-infrastructure.yml
     openstack-ansible setup-openstack.yml
   #+end_src   

 üê¢ : Once the playbooks have been fully executed, you can run individual
   playbooks. For example,

   #+begin_src bash
     cd /opt/openstack-ansible/playbooks
     openstack-ansible os-keycloak-install.yml
   #+end_src

*** not all in one (normal steps)
Usually, there're 5 steps.

1. Prepare *deployment host* (ü¶ú : only one)
2. Prepare *target hosts* (many)
3. Configure deployment
4. Run playbooks
5. Verify OpenStack operation

üê¢ : We explain it one-by-one

*** Interact with the cloud

The horizon web interface provides a graphical user interface (GUI) for
interacting with the AIO deployment. Be default, it listens on port 443 (or 80,
if SSL certificate configuration was disabled). So, simply browse to the IP of
the host.

ü¶ú : Where are the user and password?

üê¢ : By default, there's user called ~admin~. Password is the
~keystone_auth_admin_password~ in ~/etc/openstack_deploy/user_secrets.yml~.

*** SSL issues
By default, the deploying host uses self-signed certificates. But these are not
trusted by the client host. To fix this:

1. copy the certificate to the client host. The name and location of the
   certificate of the generated certificate are configured by the
   ~pki_authorities~ and ~pki_trust_store_location~ variables (default to
   ~ExampleCorpRoot~). So you might wanna do something like:

   #+begin_src bash
     scp aio:/usr/local/share/ca-certificates/ExampleCorpRoot.crt ~/.config/openstack/aio.crt
   #+end_src

   And next, in your ~cloud.yaml~:
   #+begin_src yaml
     clouds:
       aio:
         # ...
         cacert: /home/<username>/.config/openstack/aio.crt
   #+end_src
 
** Openstack client
*** install
#+begin_src bash
  pip install python-openstackclient
#+end_src
*** get started

*** help
#+begin_src bash
  cmd=(address group create)
  openstack help $cmd
#+end_src
**** available commands
  access rule delete  Delete access rule(s)
  ...
  volume type unset  Unset volume type properties
  volume unset  Unset volume properties

*** global options
openstack takes global options. Most have a corresponding environment variable,
but the command-line option takes priority. For example:

+ --os-cloud <cloud-name> ::  look for ~clouds.yaml~
+ --os-auth-url <auth-url>
+ --os-auth-type <auth-type>
+ --os-region-name :: Authentication region name

*** cloud configuration
opensatck will look for a file called ~clouds.yaml~ in the following locations:

+ current folder
+ ~/.config/openstack
+ /etc/openstack

The first one found will be used.

The structure is something like:
#+begin_src yaml
  clouds:
    my_cloud1:
      k1 : v1
      k2 : v2
    my_cloud2:
      k1 : v1
      k2 : v2
#+end_src

Where k1, k2 match the openstack global options but without the ~os-~ prefixÔºö
#+begin_src yaml
  clouds:
    devstack:
      auth:
        auth_url: http://192.168.122.10:5000/
        project_name: demo
        username: demo
        password: 0penstack
      region_name: RegionOne
    ds-admin:
      auth:
        auth_url: http://192.168.122.10:5000/
        project_name: admin
        username: admin
        password: 0penstack
      region_name: RegionOne
    infra:
      cloud: rackspace
      auth:
        project_id: 275610
        username: openstack
        password: xyzpdq!lazydog
      region_name: DFW,ORD,IAD
#+end_src

ü¶ú : What? So ~region_name~ should correspond to ~os-region-name~, ~cloud~
should correspond to ~os-cloud~, but why is ~auth~ different? There are
subfields under it.

üê¢ : The official document just said that the ~auth_url~ for the ~rackspace~
cloud is taken from ~clouds-public.yaml~.:
#+begin_src yaml
  public-clouds:
    rackspace:
      auth:
        auth_url: 'https://identity.api.rackspacecloud.com/v2.0/'
      #+end_src

      To be honest. I don't get that either. But I guess the cloud providers
      will let us know.




*** create security group
Open the tcp ports 8774,...,8790 (which are the service ports for an openstack
AIO deployment.)
#+begin_src bash
  openstack security group create openstack-apis \
      --description 'Allow access to various OpenStack services'
  for port in 8774 8776 9292 9696 5000 8780; do
      openstack security group rule create openstack-apis \
        --protocol tcp --dst-port ${port}:${port} --remote-ip 0.0.0.0/0
    done
  openstack server add security group $SERVER openstack-apis
#+end_src

* End

# Local Variables:
# org-what-lang-is-for: "bash"
# End:
