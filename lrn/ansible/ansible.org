** ansible
*** hi
**** hi
1. install
#+begin_src bash
python3 -m pip install --user ansible
#+end_src

#+begin_src bash
  # just install in the venv
  pip install ansible
#+end_src

2. create an inventory by adding the IP address in ~/etc/ansible/hosts~.

   ü¶ú : I don't want to pollute the  ~/etc/~ folder. Is there any other ways to
   do it ?
   üê¢ : Yeah, see next section.

#+begin_src yaml
  [myvirtualmachines]
  192.0.2.50
  192.0.2.51
  192.0.2.52 
#+end_src

3. verify the hosts

ansible all --list-hosts

4. Set up SSH connections so Ansible can connect to the managed hosts.

   a. Add the public SSH key to the `authorized_keys` file on each remote
   system.
   b. Test the SSH connections.

   #+begin_src bash
     ssh me@192.0.2.50
   #+end_src

   If the username on the control node is different, you need to pass the ~-u~
   option with the ~ansible~ command.

5. Ping the managed hosts.

   #+begin_src bash
     ansible all -m ping
   #+end_src
**** create inventory
1. Create the inventory file ~inventory.yaml~ in any folder:
#+begin_src yaml
  myvms:
    hosts:
      vm01:
        ansible_host: 192.0.2.50
      vm02:
        ansible_host: 192.0.2.51
      vm03:
        ansible_host: 192.0.2.52
#+end_src

Add a new group for your hosts then specify the IP address or fully qualified
domain name (FQDN) of managed node in the group with ~ansible_host~ field.

2. Verify your inventory.

   #+begin_src bash
     ansible-inventory -i inventory.yaml --list
   #+end_src

3. ping the managed hosts.

   #+begin_src bash
     ansible myvms -i inventory.yaml -m ping
   #+end_src

üê¢ : Note:

+ Ensure that group names are meaningful and unique. They are case-sensitive.
+ Invalid group names include "I have space", "I-have-hyphens", "1IhavePrecedingNum"
**** variables
Variables are like the args passed to the ansible command.

They can be local

#+begin_src yaml
 webservers:
  hosts:
    webserver01:
      ansible_host: 192.0.2.140
      http_port: 80
    webserver02:
      ansible_host: 192.0.2.150
      http_port: 443 
#+end_src

Or group-scoped

#+begin_src yaml
  webservers:
  hosts:
    webserver01:
      ansible_host: 192.0.2.140
      http_port: 80
    webserver02:
      ansible_host: 192.0.2.150
      http_port: 443
  vars:
    ansible_user: my_server_user
#+end_src
**** playbook
üê¢ : playbook is like the startup script for the managed hosts.

+ play :: an ordered list of *tasks* to execute against nodes in an *inventory*.
+ task :: A list of one or more *modules* that defines the operations to be
  performed by Ansible. that Ansible performs.
+ module :: a unit of code or binary that Ansible runs on managed nodes.

1. Create the playbook to print "Hello world".

playbook.yaml:
#+begin_src yaml
  - name: My first play
    hosts: virtualmachines
    tasks:
     - name: Ping my hosts
       ansible.builtin.ping:

     - name: Print message
       ansible.builtin.debug:
         msg: Hello world
#+end_src

2. Run the playbook.

inventory.yaml:
   #+begin_src bash
     ansible-playbook -i inventory.yaml playbook.yaml
   #+end_src
*** inventory
**** implicit group: ~all~
All nodes have at least two groups: ~all~ and its own group, (or ~ungrouped~
group, for orphan nodes).
ü¶ú : Oh, so we can just do something like ping all the nodes.
#+begin_src bash
  ansible all -i my-first-inventory.yaml -m ping
#+end_src


** OpenStack Ansible
*** set mirror
**** pypi
On all hosts:

/etc/pip.conf 
#+begin_src conf
[global]
index-url = http://pip.example.org/simple
#+end_src

/root/.pydistutils.cfg 
#+begin_src conf
[easy_install]
index_url = https://pip.example.org/simple   
#+end_src

Then in /etc/openstack_deploy/user_variables.yml:
#+begin_src yaml
  # Copy these files from the host into the containers
  lxc_container_cache_files_from_host:
    - /etc/pip.conf
    - /root/.pydistutils.cfg 
#+end_src
**** lxc
ü¶ú : in the ~lxc_hosts~ role, I see
#+begin_src yaml
lxc_ubuntu_mirror: "{{ (ansible_facts['architecture'] == 'x86_64') | ternary('http://archive.ubuntu.com/ubuntu', 'http://ports.ubuntu.com/ubuntu-ports') }}"
lxc_apt_mirror: "{{ (ansible_facts['distribution'] == 'Ubuntu') | ternary(lxc_ubuntu_mirror, 'http://deb.debian.org/debian') }}" 
#+end_src

*** all in one [for testing]
ü¶ú : All the following commands should be run with ~sodo~.
1. Prepare the host
   #+begin_src bash
     apt update
     apt dist-upgrade
     reboot
   #+end_src

2. bootstrap Ansible and the required roles.
#+begin_src bash
  git clone https://opendev.org/openstack/openstack-ansible /opt/openstack-ansible
  cd /opt/openstack-ansible
#+end_src

3. checkout the appropriate branch
   #+begin_src bash
     git checkout 27.0.1
   #+end_src

4. run the bootstrap-ansible script
   #+begin_src bash
     TMPDIR=/var/tmp scripts/bootstrap-ansible.sh
   #+end_src
  ü¶ú : There's a lot of configurable options here, but I am gonna stick to the
   default for now.

5. run the bootstrap-aio script
    #+begin_src bash
      scripts/bootstrap-aio.sh
    #+end_src

6. run the playbooks
   #+begin_src bash
     cd /opt/openstack-ansible/playbooks
     openstack-ansible setup-hosts.yml
     openstack-ansible setup-infrastructure.yml
     openstack-ansible setup-openstack.yml
   #+end_src   

 üê¢ : Once the playbooks have been fully executed, you can run individual
   playbooks. For example,

   #+begin_src bash
     cd /opt/openstack-ansible/playbooks
     openstack-ansible os-keycloak-install.yml
   #+end_src

*** not all in one (normal steps)
Usually, there're 5 steps.

1. Prepare *deployment host* (ü¶ú : only one)
2. Prepare *target hosts* (many)
3. Configure deployment
4. Run playbooks
5. Verify OpenStack operation

üê¢ : We explain it one-by-one

*** Interact with the cloud

The horizon web interface provides a graphical user interface (GUI) for
interacting with the AIO deployment. Be default, it listens on port 443 (or 80,
if SSL certificate configuration was disabled). So, simply browse to the IP of
the host.

ü¶ú : Where are the user and password?

üê¢ : By default, there's user called ~admin~. Password is the
~keystone_auth_admin_password~ in ~/etc/openstack_deploy/user_secrets.yml~.

*** SSL issues
By default, the deploying host uses self-signed certificates. But these are not
trusted by the client host. To fix this:

1. copy the certificate to the client host. The name and location of the
   certificate of the generated certificate are configured by the
   ~pki_authorities~ and ~pki_trust_store_location~ variables (default to
   ~ExampleCorpRoot~). So you might wanna do something like:

   #+begin_src bash
     scp aio:/usr/local/share/ca-certificates/ExampleCorpRoot.crt ~/.config/openstack/aio.crt
   #+end_src

   And next, in your ~cloud.yaml~:
   #+begin_src yaml
     clouds:
       aio:
         # ...
         cacert: /home/<username>/.config/openstack/aio.crt
   #+end_src
 

** Openstack client
*** install
#+begin_src bash
  pip install python-openstackclient
#+end_src
*** get started

*** help
#+begin_src bash
  cmd=(address group create)
  openstack help $cmd
#+end_src
**** available commands
  access rule delete  Delete access rule(s)
  ...
  volume type unset  Unset volume type properties
  volume unset  Unset volume properties

*** global options
openstack takes global options. Most have a corresponding environment variable,
but the command-line option takes priority. For example:

+ --os-cloud <cloud-name> ::  look for ~clouds.yaml~
+ --os-auth-url <auth-url>
+ --os-auth-type <auth-type>
+ --os-region-name :: Authentication region name

*** cloud configuration
opensatck will look for a file called ~clouds.yaml~ in the following locations:

+ current folder
+ ~/.config/openstack
+ /etc/openstack

The first one found will be used.

The structure is something like:
#+begin_src yaml
  clouds:
    my_cloud1:
      k1 : v1
      k2 : v2
    my_cloud2:
      k1 : v1
      k2 : v2
#+end_src

Where k1, k2 match the openstack global options but without the ~os-~ prefixÔºö
#+begin_src yaml
  clouds:
    devstack:
      auth:
        auth_url: http://192.168.122.10:5000/
        project_name: demo
        username: demo
        password: 0penstack
      region_name: RegionOne
    ds-admin:
      auth:
        auth_url: http://192.168.122.10:5000/
        project_name: admin
        username: admin
        password: 0penstack
      region_name: RegionOne
    infra:
      cloud: rackspace
      auth:
        project_id: 275610
        username: openstack
        password: xyzpdq!lazydog
      region_name: DFW,ORD,IAD
#+end_src

ü¶ú : What? So ~region_name~ should correspond to ~os-region-name~, ~cloud~
should correspond to ~os-cloud~, but why is ~auth~ different? There are
subfields under it.

üê¢ : The official document just said that the ~auth_url~ for the ~rackspace~
cloud is taken from ~clouds-public.yaml~.:
#+begin_src yaml
  public-clouds:
    rackspace:
      auth:
        auth_url: 'https://identity.api.rackspacecloud.com/v2.0/'
      #+end_src

      To be honest. I don't get that either. But I guess the cloud providers
      will let us know.




*** create security group
Open the tcp ports 8774,...,8790 (which are the service ports for an openstack
AIO deployment.)
#+begin_src bash
  openstack security group create openstack-apis \
      --description 'Allow access to various OpenStack services'
  for port in 8774 8776 9292 9696 5000 8780; do
      openstack security group rule create openstack-apis \
        --protocol tcp --dst-port ${port}:${port} --remote-ip 0.0.0.0/0
    done
  openstack server add security group $SERVER openstack-apis
#+end_src

* End

# Local Variables:
# org-what-lang-is-for: "bash"
# End:
